import os
import re
import io
import sys
import json
import math
import time
import pdfplumber
import streamlit as st
from typing import List, Dict, Any, Tuple
from pathlib import Path

# --- Chroma on Streamlit Cloud needs sqlite shim ---
__import__("pysqlite3")
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import chromadb
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
from openai import OpenAI

# ---------------- UI ----------------
st.set_page_config(page_title="SEC 10-Q RAG Bot", page_icon="ðŸ“Š", layout="wide")
st.title("ðŸ“Š IST 688 â€” SEC 10-Q RAG + Re-Ranking Chatbot")

# ---------------- Secrets / Clients ----------------

# ---------------- Secrets / Clients ----------------
import os
import streamlit as st
from openai import OpenAI

# Read only from Streamlit secrets
raw_key = st.secrets.get("OPENAI_API_KEY", "")


# Normalize: strip whitespace/newlines that sometimes sneak in via secrets UI
OPENAI_KEY = (raw_key or "").strip()

if not OPENAI_KEY:
    st.error("Missing OPENAI_API_KEY in Streamlit secrets.")
    st.stop()

# Validate key type and required fields
# Optional: small self-test to fail fast with a clear error
try:
    oc = OpenAI(
        api_key=OPENAI_KEY,
        organization=org_id if is_project_key else None,
        project=proj_id if is_project_key else None,
        timeout=60,
        max_retries=2,
    )
    # light ping; avoids heavy calls
    _ = oc.models.list()  # if unauthorized, raises AuthenticationError
except Exception as e:
    st.error(f"OpenAI auth check failed: {e}")
    st.stop()

# Make key available to libs that read env internally
os.environ["OPENAI_API_KEY"] = OPENAI_KEY
if is_project_key:
    os.environ["OPENAI_ORG_ID"] = org_id
    os.environ["OPENAI_PROJECT_ID"] = proj_id

# ---------------- Paths / Constants ----------------
PERSIST_DIR = "sec10q_chroma"
COLLECTION  = "sec10q_collection"
MAX_CHUNK_CHARS = 1800   # for display + prompt safety
CHUNK_SIZE = 1400        # characters
CHUNK_OVERLAP = 200      # characters

Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)

# ---------------- Helpers ----------------
def _clean_text(t: str) -> str:
    t = re.sub(r"\s+", " ", t or " ")
    return t.strip()

def _chunk_text(text: str, company: str, source_name: str, page_no: int) -> List[Dict[str, Any]]:
    text = text.replace("\r", "")
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + CHUNK_SIZE, n)
        chunk = text[start:end].strip()
        if chunk:
            chunks.append({
                "content": chunk,
                "metadata": {
                    "company": company,
                    "source": source_name,
                    "page": page_no
                }
            })
        if end == n:
            break
        start = max(end - CHUNK_OVERLAP, 0)
    return chunks

def _extract_pdf_chunks(file: io.BytesIO, filename: str, company_hint: str = "") -> List[Dict[str, Any]]:
    """Extract per-page text via pdfplumber and chunk."""
    chunks = []
    company = company_hint or (
        "Amazon" if "amazon" in filename.lower() or "amzn" in filename.lower() else
        "Apple"  if "apple"  in filename.lower() or "aapl" in filename.lower() else
        "Unknown"
    )
    with pdfplumber.open(file) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            try:
                txt = page.extract_text() or ""
            except Exception:
                txt = ""
            txt = _clean_text(txt)
            if not txt:
                continue
            page_chunks = _chunk_text(txt, company, filename, i)
            chunks.extend(page_chunks)
    return chunks

# ---------------- Vector DB (Chroma) ----------------
@st.cache_resource(show_spinner=True)
def _get_collection():
    client = chromadb.PersistentClient(path=PERSIST_DIR)
    embedder = OpenAIEmbeddingFunction(api_key=OPENAI_KEY, model_name="text-embedding-3-small")
    col = client.get_or_create_collection(
        name=COLLECTION,
        embedding_function=embedder,
        metadata={"hnsw:space": "cosine"}
    )
    return col

collection = _get_collection()

def _already_indexed_ids():
    try:
        got = collection.get()
        return set(got.get("ids", []))
    except Exception:
        return set()

def _add_chunks_to_chroma(doc_chunks: List[Dict[str, Any]], doc_key: str):
    existing = _already_indexed_ids()
    ids, docs, metas = [], [], []
    added = 0
    for idx, ch in enumerate(doc_chunks):
        cid = f"{doc_key}_{idx:04d}"
        if cid in existing:
            continue
        ids.append(cid)
        docs.append(ch["content"])
        metas.append(ch["metadata"])
        added += 1
    if ids:
        collection.add(ids=ids, documents=docs, metadatas=metas)
    return added

# ---------------- Sidebar: Upload & Controls ----------------
with st.sidebar:
    st.header("Step 1 â€” Upload 10-Q PDFs")
    uploads = st.file_uploader("Upload Amazon / Apple 10-Q PDFs", type=["pdf"], accept_multiple_files=True)
    company_override = st.text_input("Optional company override (applies to *all* new uploads)", value="")
    if st.button("Process PDFs"):
        if not uploads:
            st.warning("Please upload at least one PDF.")
        else:
            total_added = 0
            for up in uploads:
                chunks = _extract_pdf_chunks(up, up.name, company_override.strip())
                added = _add_chunks_to_chroma(chunks, doc_key=up.name)
                total_added += added
            st.success(f"Indexed {total_added} chunks. (New only; duplicates are skipped.)")

    st.header("Retrieval")
    k_retrieve = st.slider("Max chunks to retrieve", 3, 30, 8, step=1)
    use_llm_rerank = st.checkbox("Use LLM re-ranking (reorder retrieved chunks)", value=False)
    company_filter = st.multiselect("Filter by company (optional)", ["Amazon", "Apple", "Unknown"])

# ---------------- Retrieval ----------------
def retrieve_chunks(query: str, n_results: int, company_filter: List[str]) -> List[Dict[str, Any]]:
    res = collection.query(
        query_texts=[query],
        n_results=n_results,
        include=["documents", "metadatas", "distances"]
    )
    docs = (res.get("documents") or [[]])[0]
    metas = (res.get("metadatas") or [[]])[0]
    dists = (res.get("distances") or [[]])[0]

    results = []
    for text, meta, dist in zip(docs, metas, dists):
        if company_filter and meta.get("company") not in company_filter:
            continue
        results.append({
            "content": text,
            "metadata": meta,
            "distance": dist
        })
    return results

def llm_rerank(query: str, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Ask the LLM to score each chunk for relevance (1â€“10), return reordered list.
    JSON-only response; robust parsing.
    """
    pack = []
    for i, it in enumerate(items):
        meta = it["metadata"]
        pack.append({
            "id": i,
            "company": meta.get("company",""),
            "source": meta.get("source",""),
            "page": meta.get("page", 0),
            "snippet": it["content"][:MAX_CHUNK_CHARS],
        })

    instr = (
        "You are ranking 10-Q text chunks for their relevance to the user's query.\n"
        "Score each item from 1 (not relevant) to 10 (highly relevant). "
        "Prefer chunks that directly discuss the user's query and contain concrete numbers or MD&A content. "
        "Return STRICT JSON: [{id, score}]. No extra text."
    )
    prompt = [
        {"role":"system","content": instr},
        {"role":"user","content": f"Query: {query}\n\nChunks:\n{json.dumps(pack)}"}
    ]
    try:
        resp = oc.chat.completions.create(
            model="gpt-4o-mini",
            messages=prompt,
            temperature=0.2
        )
        txt = resp.choices[0].message.content.strip()
        m = re.search(r"\[.*\]", txt, flags=re.DOTALL)
        arr = json.loads(m.group(0) if m else txt)
        by_id = {int(a["id"]): float(a.get("score", 5)) for a in arr if "id" in a}
    except Exception:
        by_id = {i: 5.0 for i in range(len(items))}

    # Attach score and sort desc
    ranked = []
    for i, it in enumerate(items):
        it2 = dict(it)
        it2["_llm_score"] = by_id.get(i, 5.0)
        ranked.append(it2)
    ranked.sort(key=lambda x: x.get("_llm_score", 0), reverse=True)
    return ranked

# ---------------- Answer Generation ----------------
SYSTEM_PROMPT = (
    "You are a helpful financial analysis assistant for SEC 10-Q filings. "
    "Use ONLY the provided CONTEXT chunks when possible. "
    "If answering, cite sources as [Company, page N]. "
    "Be concise and neutral; include key figures if present."
)

def answer_with_context(query: str, context_items: List[Dict[str, Any]]) -> str:
    # Build a compact context string
    ctx_blocks = []
    for it in context_items:
        meta = it["metadata"]
        tag = f"[{meta.get('company','?')}, page {meta.get('page','?')}]"
        ctx_blocks.append(f"{tag} {it['content'][:MAX_CHUNK_CHARS]}")
    context_text = "\n\n---\n\n".join(ctx_blocks)

    messages = [
        {"role":"system","content": SYSTEM_PROMPT},
        {"role":"system","content": f"CONTEXT:\n{context_text}"},
        {"role":"user","content": query}
    ]
    stream = oc.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        stream=True,
        temperature=0.2
    )
    return stream  # Streamlit can consume this

# ---------------- Chat UI ----------------
st.header("Step 2 â€” Ask Retrieval Questions")
default_q = "Summarize the financial performance this quarter."
user_q = st.text_input("Your question:", value=default_q, placeholder="e.g., What are the main risks?")
run = st.button("Run")

# A place to show retrieved sources
results_container = st.container()
answer_container = st.container()

if run:
    with st.spinner("Retrieving relevant chunksâ€¦"):
        raw_hits = retrieve_chunks(user_q, n_results=max(30, k_retrieve), company_filter=company_filter)
        # keep first k before optional re-rank (mimic â€œmax chunks to retrieveâ€)
        hits = raw_hits[:k_retrieve]

        if use_llm_rerank and hits:
            hits = llm_rerank(user_q, hits)

    with results_container:
        st.subheader("Retrieved Chunks (top to bottom):")
        for i, it in enumerate(hits, start=1):
            meta = it["metadata"]
            tag = f"{meta.get('company','?')} â€” {meta.get('source','')}, p.{meta.get('page','?')}"
            cols = st.columns([0.1, 0.9])
            cols[0].markdown(f"**{i}.**")
            cols[1].markdown(f"**{tag}**")
            cols[1].write(it["content"][:MAX_CHUNK_CHARS] + ("â€¦" if len(it["content"])>MAX_CHUNK_CHARS else ""))
            if "_llm_score" in it:
                st.caption(f"LLM relevance score: {it['_llm_score']:.1f}")

    with answer_container:
        st.subheader("Answer")
        with st.chat_message("assistant"):
            stream = answer_with_context(user_q, hits)
            final_text = st.write_stream(stream)

